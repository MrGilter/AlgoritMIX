<?xml version="1.0" encoding="utf-8" ?>
<root>
  <Compresion name="Arithmetic">
    <text>
      Пpи аpифметическом кодиpовании, в отличие от рассмотренных нами методов, когда кодируемый символ (или группа символов) заменяется соответствующим им кодом, результат кодирования всего сообщения пpедставляется одним или парой вещественных чисел в интеpвале от 0 до 1 . По меpе кодиpования исходного текста отобpажающий его интеpвал уменьшается, а количество десятичных (или двоичных) разрядов, служащих для его пpедставления, возpастает. Очеpедные символы входного текста сокpащают величину интеpвала исходя из значений их веpоятностей, определяемых моделью. Более веpоятные символы делают это в меньшей степени, чем менее веpоятные, и, следовательно, добавляют меньше разрядов к pезультату.

      Пример постоянной модели для алфавита { a,e,i,o,u,! }
      Символ	Вероятность	Интервал
      a		.2		[0.0; 0.2)
      e		.3		[0.2; 0.5)
      i		.1		[0.5; 0.6)
      o		.2		[0.6; 0.8)
      u		.1		[0.8; 0.9)
      !		.1		[0.9; 1.0)

      И кодировщику, и декодировщику известно, что в самом начале интервал есть [0; 1). После просмотра первого символа "e", кодировщик сужает интервал до [0.2; 0.5), который модель выделяет этому символу. Второй символ "a" сузит этот новый интервал до первой его пятой части, поскольку для "a" выделен фиксированный интервал [0.0; 0.2). В результате получим рабочий интервал [0.2; 0.26), т.к. предыдущий интервал имел ширину в 0.3 единицы и одна пятая от него есть 0.06. Следующему символу "i" соответствует фиксированный интервал [0.5; 0.6), что применительно к рабочему интервалу [0.2; 0.26) суживает его до интервала [0.23, 0.236).

      Продолжая в том же духе, имеем:

      В начале	    	  	[0.0; 1.0 )
      После просмотра   	"e";	  [0.2; 0.5 )
      После просмотра	"a"	  [0.2; 0.26 )
      После просмотра	"i"	  [0.23; 0.236 )
      После просмотра	"i"	  [0.233; 0.2336)
      После просмотра	"!"	  [0.23354; 0.2336)


      Вся суть алгоритма арифметического кодирования заключаеться в следующих шагах:
      1) Изначально у нас есть слово(текст) которое надо закодировать, и для етого слова мы строим таблицу вероятностей появления символов.
      2) В зависимости от вероятности разпределяеться длинна отрезка для каждого из символов в диапазоне [0; 1)
      3) Допустим у нас есть кодируемое слово SWISS_MISS и для нее мы построили следующую таблицу:

      Символ  	Вероятность	Интервал
      -		    1/10 = 0.1	[0,0, 0,1)
      M		    1/10 = 0,1	[0,1, 0,2)
      I		    2/10 = 0,2	[0,2, 0,4)
      W		    1/10 = 0,1	[0,4, 0,5)
      S	  	  5/10 = 0,5	[0,5, 1,0)

      Процесс кодирования начинается с инциализации двух переменных Low и High значениями 0 и 1 соответственно. Они определяют интервал [Low, High). По мере поступления и обработки символов значения переменных Low и High начинают сближаться, уменьшая длину интервала. С каждым поступившим символом переменные Low и High пересчитываются по правилу:

      Range = OldHigh­ – OldLow,

      NewHigh = OldLow + Range*HighRange(x),

      NewLow = OldLow + Range*LowRange(x),

      где HighRange(x) и LowRange(x) – верхняя и нижняя границы интервала символа x. Процесс кодирования показан ниже. Результирующий код – последнее значение переменной Low, равное 0,71753375, из которого последние восемь цифр (71753375) записываются в сжатый файл.

      |Символ	|Переменная	|Вычисление				| Результат
      |	S	|	L	|	0,0+(1,0-0,0)∙0,5			|	0,5
      |		|	H	|	0,0+(1,0-0,0)∙1,0			|	1,0
      |	W	|	L	|	0,5+(1,0-0,5)∙0,4			|	0,7
      |		|	H	|	0,5+(1,0-0,5)∙0,5			|	0,75
      |	I	|	L	|	0,7+(0,75-0,7)∙0,2			|	0,71
      |		|	H	|	0,7+(0,75-0,7)∙0,4			|	0,72
      |	S	|	L	|	0,71+(0,72-0,71)∙0,5		|	0,715
      |		|	H	|	0,71+(0,72-0,71)∙1,0		|	0,72
      |	S	|	L	|	0,715+(0,72-0,715)∙0,5		|	0,7175
      |		|	H	|	0,715+(0,72-0,715)∙1,0		|	0,72
      |	_	|	L	|	0,7175+(0,72-0,7175)∙0,0		|	0,7175
      |		|	H	|	0,7175+(0,72-0,7175)∙0,1		|	0,71775
      |	M	|	L	|	0,7175+(0,71775-0,7175)∙0,1		|	0,717525
      |		|	H	|	0,7175+(0,71775-0,7175)∙0,2		|	0,717550
      |	I	|	L	|	0,717525+(0,71755-0,717525)∙0,2	|	0,717530
      |		|	H	|	0,717525+(0,71755-0,717525)∙0,4	|	0,717535
      |	S	|	L	|	0,717530+(0,717535-0,717530)∙0,5	|	0,7175325
      |		|	H	|	0,717530+(0,717535-0,717530)∙1,0	|	0,7175325
      |	S	|	L	|	0,7175325+(0,717535-0,7175325)∙0,5	|	0,71753375
      |		|	H	|	0,7175325+(0,717535-0,7175325)∙1,0	|	0,717535

      Таким образом любое число с интервала 0,71753375 ... 0,717535 являеться нашим закодированным словом.

      4) Декодирование:
      Декодер работает в обратном порядке. Сначала считывается статистическая модель, содержащаяся в таблице 12.2. Затем считывается цифра 7, соответствующая коду 0,7. Это число лежит внутри интервала [0,5, 1), поэтому первым декодированным символом является символ S. Далее удаляется эффект символа S путём вычитания нижнего конца интервала S из текущего значения кода и деления разности на длину этого интервала. Результатом будет число 0,4350675, которое говорит о том, что второй декодируемый символ – W (интервал символа W – [0,4, 0,5)). Процесс продолжается до тех пор, пока текущее значение кода не станет равным 0 (или мы не натикаемся на наш специальный символ eof)

      Тоесть формула следующая:
      Code  - Закодированная последовательность ( в нашем случае число с интервала 0,71753375 ... 0,717535)
      Low   - Минимальная граница
      Hight - Максимальная граница
      item  - Элемент таблицы интервалов (можно разсматривать как элемент ключем которого являеться кодируемый символ)
      current - указатель на текуший элемент

      (Code - item[current].Low)/(item[current].Hight - item[current].Low) = код следующего символа

      В теории класический алгоритм довольно интересен но на практике он не дает возможность закодировать длинную последовательность символов в строке из за ограничения дробных разрядов будь тип float или double... Так же на практике при попытке закодировать длинные или сложные(с большим алфавитом) строки в итоге получаеться не правильный код, который при декодировке не даст оригинальной строки.




    </text>
  </Compresion>
  <Compresion name="Huffman">
    <text>
      Классический алгоритм Хаффмана на входе получает таблицу частот встречаемости символов в сообщении. Далее на основании этой таблицы строится дерево кодирования Хаффмана.

      1) Символы входного алфавита образуют список свободных узлов. Каждый лист имеет вес, который может быть равен либо вероятности, либо количеству вхождений символа в сжимаемое сообщение.
      2) Выбираются два свободных узла дерева с наименьшими весами.
      3) Создается их родитель с весом, равным их суммарному весу.
      4) Родитель добавляется в список свободных узлов, а два его потомка удаляются из этого списка.
      5) Одной дуге, выходящей из родителя, ставится в соответствие бит 1, другой — бит 0. Битовые значения ветвей, исходящих от корня, не зависят от весов потомков.
      6) Шаги, начиная со второго, повторяются до тех пор, пока в списке свободных узлов не останется только один свободный узел. Он и будет считаться корнем дерева.
      7) После того как мы сформировали дерево, мы его можем использовать для кодирования данных. Кодирование заключается в том, чтобы пройти от корневого элемента дерева к каждому элементу исходной отсортированной таблицы (к каждому листу), и на каждом “повороте”, если “поворачиваем” влево — запоминаем 0-й бит, а если вправо — 1-й бит. Изобразим “дорожку” к каждому листу дерева, начиная из корня, и определим код каждой нашей буквы.

      Для декодирования декодеру необходимо занть таблицу весов.

      Классический алгоритм Хаффмана имеет ряд существенных недостатков. Во-первых, для восстановления содержимого сжатого сообщения декодер должен знать таблицу частот, которой пользовался кодер. Следовательно, длина сжатого сообщения увеличивается на длину таблицы частот, которая должна посылаться впереди данных, что может свести на нет все усилия по сжатию сообщения. Кроме того, необходимость наличия полной частотной статистики перед началом собственно кодирования требует двух проходов по сообщению: одного для построения модели сообщения (таблицы частот и Н-дерева), другого для собственно кодирования. Во-вторых, избыточность кодирования обращается в ноль лишь в тех случаях, когда вероятности кодируемых символов являются обратными степенями числа 2. В-третьих, для источника с энтропией, не превышающей 1 (например, для двоичного источника), непосредственное применение кода Хаффмана бессмысленно.
    </text>
  </Compresion>
</root>